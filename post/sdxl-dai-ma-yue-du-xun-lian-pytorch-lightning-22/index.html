<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>SDXL代码阅读-训练(pytorch lightning)-2.2  | 谢狗个人博客</title>
<link rel="shortcut icon" href="https://ShiMinghao0208.github.io/favicon.ico?v=1727341370783">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://ShiMinghao0208.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="SDXL代码阅读-训练(pytorch lightning)-2.2  | 谢狗个人博客 - Atom Feed" href="https://ShiMinghao0208.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="上一节给PL开了个头，简单讲了一下PL构建一整套模型及训练系统需要写什么，这篇文章继续。
起始参考代码还是generative-models/blob/main/main.py，见662行model = instantiate_from_c..." />
    <meta name="keywords" content="SDXL,AIGC学习" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://ShiMinghao0208.github.io">
  <img class="avatar" src="https://ShiMinghao0208.github.io/images/avatar.png?v=1727341370783" alt="">
  </a>
  <h1 class="site-title">
    谢狗个人博客
  </h1>
  <p class="site-description">
    我不姓谢，但最怕万一见温柔
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="https://ShiMinghao0208.github.io/tags" class="menu">
          归档
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              SDXL代码阅读-训练(pytorch lightning)-2.2 
            </h2>
            <div class="post-info">
              <span>
                2024-09-20
              </span>
              <span>
                16 min read
              </span>
              
                <a href="https://ShiMinghao0208.github.io/tag/354-myU_X/" class="post-tag">
                  # SDXL
                </a>
              
                <a href="https://ShiMinghao0208.github.io/tag/V3dvREVpE/" class="post-tag">
                  # AIGC学习
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p><a href="https://shiminghao0208.github.io/post/sdxl-dai-ma-yue-du-xun-lian-pytorch-lightning-21/">上一节</a>给PL开了个头，简单讲了一下PL构建一整套模型及训练系统需要写什么，这篇文章继续。<br>
起始参考代码还是generative-models/blob/main/main.py，见662行model = instantiate_from_config(config.model)结合上一节解析的SDXL是如何动态导入模块的，那么这里我们可以直接去配置文件看看它到底是怎么进行model的构造的。</p>
<h1 id="model">Model</h1>
<p>还是以generative-models/configs/example_training/toy/mnist_cond.yaml配置文件作为参考，发现model构造来自于sgm.models.diffusion.DiffusionEngine方法。</p>
<h2 id="diffusionengine">DiffusionEngine</h2>
<p>通过配置文件是多层target嵌套可以知道DiffusionEngine是由多个module组合init的，参考init代码也可以得知由以下几个模块组成：<br>
1.network<br>
2.denoiser<br>
3.sampler<br>
4.loss_fn<br>
5.conditioner</p>
<pre><code class="language-python3">class DiffusionEngine(pl.LightningModule):
    def __init__(
        self,
        network_config,
        denoiser_config,
        first_stage_config,
        conditioner_config: Union[None, Dict, ListConfig, OmegaConf] = None,
        sampler_config: Union[None, Dict, ListConfig, OmegaConf] = None,
        optimizer_config: Union[None, Dict, ListConfig, OmegaConf] = None,
        scheduler_config: Union[None, Dict, ListConfig, OmegaConf] = None,
        loss_fn_config: Union[None, Dict, ListConfig, OmegaConf] = None,
        network_wrapper: Union[None, str] = None,
        ckpt_path: Union[None, str] = None,
        use_ema: bool = False,
        ema_decay_rate: float = 0.9999,
        scale_factor: float = 1.0,
        disable_first_stage_autocast=False,
        input_key: str = &quot;jpg&quot;,
        log_keys: Union[List, None] = None,
        no_cond_log: bool = False,
        compile_model: bool = False,
        en_and_decode_n_samples_a_time: Optional[int] = None,
    ):
        super().__init__()
        self.log_keys = log_keys
        self.input_key = input_key
        self.optimizer_config = default(
            optimizer_config, {&quot;target&quot;: &quot;torch.optim.AdamW&quot;}
        )
        model = instantiate_from_config(network_config)
        self.model = get_obj_from_str(default(network_wrapper, OPENAIUNETWRAPPER))(
            model, compile_model=compile_model
        )

        self.denoiser = instantiate_from_config(denoiser_config)
        self.sampler = (
            instantiate_from_config(sampler_config)
            if sampler_config is not None
            else None
        )
        self.conditioner = instantiate_from_config(
            default(conditioner_config, UNCONDITIONAL_CONFIG)
        )
        self.scheduler_config = scheduler_config
        self._init_first_stage(first_stage_config)

        self.loss_fn = (
            instantiate_from_config(loss_fn_config)
            if loss_fn_config is not None
            else None
        )

        self.use_ema = use_ema
        if self.use_ema:
            self.model_ema = LitEma(self.model, decay=ema_decay_rate)
            print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)

        self.scale_factor = scale_factor
        self.disable_first_stage_autocast = disable_first_stage_autocast
        self.no_cond_log = no_cond_log

        if ckpt_path is not None:
            self.init_from_ckpt(ckpt_path)

        self.en_and_decode_n_samples_a_time = en_and_decode_n_samples_a_time
</code></pre>
<p>整个model其实对应着DiffusionEngine，而DiffusionEngine里面初始化的model(network_config)更符合我们以往对于model的定义，对应配置文件也就是sgm.modules.diffusionmodules.openaimodel.UNetModel，后续再分步阅读。</p>
<h3 id="_init_first_stage">_init_first_stage</h3>
<p>DiffusionEngine会有一个init_first_stage的方法，用于直接copy一个network.eval且固定其权重，因为有些part是不需要训练的，比如vae的解码编码、clip等等。</p>
<pre><code class="language-python3">    def _init_first_stage(self, config):
        model = instantiate_from_config(config).eval()
        model.train = disabled_train
        for param in model.parameters():
            param.requires_grad = False
        self.first_stage_model = model
</code></pre>
<p>譬如后面的encode、decode就是调用first_stage_model</p>
<pre><code class="language-python3">    @torch.no_grad()
    def encode_first_stage(self, x):
        n_samples = default(self.en_and_decode_n_samples_a_time, x.shape[0])
        n_rounds = math.ceil(x.shape[0] / n_samples)
        all_out = []
        with torch.autocast(&quot;cuda&quot;, enabled=not self.disable_first_stage_autocast):
            for n in range(n_rounds):
                out = self.first_stage_model.encode(
                    x[n * n_samples : (n + 1) * n_samples]
                )
                all_out.append(out)
        z = torch.cat(all_out, dim=0)
        z = self.scale_factor * z
        return z

    @torch.no_grad()
    def decode_first_stage(self, z):
        z = 1.0 / self.scale_factor * z
        n_samples = default(self.en_and_decode_n_samples_a_time, z.shape[0])

        n_rounds = math.ceil(z.shape[0] / n_samples)
        all_out = []
        with torch.autocast(&quot;cuda&quot;, enabled=not self.disable_first_stage_autocast):
            for n in range(n_rounds):
                if isinstance(self.first_stage_model.decoder, VideoDecoder):
                    kwargs = {&quot;timesteps&quot;: len(z[n * n_samples : (n + 1) * n_samples])}
                else:
                    kwargs = {}
                out = self.first_stage_model.decode(
                    z[n * n_samples : (n + 1) * n_samples], **kwargs
                )
                all_out.append(out)
        out = torch.cat(all_out, dim=0)
        return out
</code></pre>
<h3 id="forward">forward</h3>
<p>forward函数与pytorch不同点在于，它将计算loss的过程放在了forward里面，因此具体要看loss_fn是怎么写的。</p>
<pre><code class="language-python3">    def forward(self, x, batch):
        loss = self.loss_fn(self.model, self.denoiser, self.conditioner, x, batch)
        loss_mean = loss.mean()
        loss_dict = {&quot;loss&quot;: loss_mean}
        return loss_mean, loss_dict
</code></pre>
<h3 id="train">train</h3>
<p>self.input_key是字符串变量，表示图片数据的格式，默认值是&quot;jpg&quot;，这里batch参数还不确定是什么，可能要具体看数据集是怎么构造的。</p>
<pre><code class="language-python3">    def get_input(self, batch):
        # assuming unified data format, dataloader returns a dict.
        # image tensors should be scaled to -1 ... 1 and in bchw format
        return batch[self.input_key]
</code></pre>
<p><strong>share step</strong><br>
真正的训练step，首先通过get_input对输入做处理，然后对input进行encode操作到潜在空间，然后调用forward函数进行前向推理。</p>
<pre><code class="language-python3">    def shared_step(self, batch: Dict) -&gt; Any:
        x = self.get_input(batch)
        x = self.encode_first_stage(x)
        # global_step 是 PyTorch Lightning 中的一个属性，用于跟踪训练过程中的全局步数。它表示从训练开始到当前已经完成的优化步骤的总数。
        # global_step 是一个自动维护的计数器，通常用于记录和监控训练过程中的指标，尤其是在日志记录和学习率调度等场景中。
        batch[&quot;global_step&quot;] = self.global_step
        loss, loss_dict = self(x, batch)
        return loss, loss_dict
</code></pre>
<p>training_step核心就是调用了share_step，其余部分基本上就是在记录log</p>
<pre><code class="language-python3">    def training_step(self, batch, batch_idx):
        loss, loss_dict = self.shared_step(batch)

        self.log_dict(
            loss_dict, prog_bar=True, logger=True, on_step=True, on_epoch=False
        )

        self.log(
            &quot;global_step&quot;,
            self.global_step,
            prog_bar=True,
            logger=True,
            on_step=True,
            on_epoch=False,
        )

        if self.scheduler_config is not None:
            lr = self.optimizers().param_groups[0][&quot;lr&quot;]
            self.log(
                &quot;lr_abs&quot;, lr, prog_bar=True, logger=True, on_step=True, on_epoch=False
            )

        return loss
</code></pre>
<h3 id="configure_optimizers">configure_optimizers</h3>
<p>在前面的训练流程里面出现了self.optimizers()，对应的就是训练过程的优化器，优化器的配置对应的就是configure_optimizers。<br>
configure_optimizers 方法在训练开始前和从检查点恢复训练时被 PyTorch Lightning 自动调用。它用于初始化优化器和学习率调度器，以便在训练过程中正确地更新模型参数和调整学习率。通过这种机制，Lightning 框架能够简化优化器和调度器的管理，使得训练过程更加高效和便捷。</p>
<pre><code class="language-python3">    def configure_optimizers(self):
        lr = self.learning_rate
        params = list(self.model.parameters())
        for embedder in self.conditioner.embedders:
            if embedder.is_trainable:
                params = params + list(embedder.parameters())
        opt = self.instantiate_optimizer_from_config(params, lr, self.optimizer_config)
        if self.scheduler_config is not None:
            scheduler = instantiate_from_config(self.scheduler_config)
            print(&quot;Setting up LambdaLR scheduler...&quot;)
            # LambdaLR通过用户定义的规则、函数来动态调整学习率
            scheduler = [
                {
                    &quot;scheduler&quot;: LambdaLR(opt, lr_lambda=scheduler.schedule),
                    &quot;interval&quot;: &quot;step&quot;,
                    &quot;frequency&quot;: 1,
                }
            ]
            return [opt], scheduler
        return opt
</code></pre>
<h3 id="sample">Sample</h3>
<p>主要就是调用denosier和sampler</p>
<pre><code class="language-python3">    @torch.no_grad()
    def sample(
        self,
        cond: Dict,
        uc: Union[Dict, None] = None,
        batch_size: int = 16,
        shape: Union[None, Tuple, List] = None,
        **kwargs,
    ):
        randn = torch.randn(batch_size, *shape).to(self.device)

        denoiser = lambda input, sigma, c: self.denoiser(
            self.model, input, sigma, c, **kwargs
        )
        samples = self.sampler(denoiser, randn, cond, uc=uc)
        return samples
</code></pre>
<h3 id="log_conditionings">log_conditionings</h3>
<p>这个函数我理解就是将各种各样的条件输入转换为一种img_size的张量并记录log</p>
<pre><code class="language-python3">    @torch.no_grad()
    def log_conditionings(self, batch: Dict, n: int) -&gt; Dict:
        &quot;&quot;&quot;
        Defines heuristics to log different conditionings.
        These can be lists of strings (text-to-image), tensors, ints, ...
        &quot;&quot;&quot;
        image_h, image_w = batch[self.input_key].shape[2:]
        log = dict()

        for embedder in self.conditioner.embedders:
            if (
                (self.log_keys is None) or (embedder.input_key in self.log_keys)
            ) and not self.no_cond_log:
                x = batch[embedder.input_key][:n]
                if isinstance(x, torch.Tensor):
                    if x.dim() == 1:
                        # class-conditional, convert integer to string
                        x = [str(x[i].item()) for i in range(x.shape[0])]
                        xc = log_txt_as_img((image_h, image_w), x, size=image_h // 4)
                    elif x.dim() == 2:
                        # size and crop cond and the like
                        x = [
                            &quot;x&quot;.join([str(xx) for xx in x[i].tolist()])
                            for i in range(x.shape[0])
                        ]
                        xc = log_txt_as_img((image_h, image_w), x, size=image_h // 20)
                    else:
                        raise NotImplementedError()
                elif isinstance(x, (List, ListConfig)):
                    if isinstance(x[0], str):
                        # strings
                        xc = log_txt_as_img((image_h, image_w), x, size=image_h // 20)
                    else:
                        raise NotImplementedError()
                else:
                    raise NotImplementedError()
                log[embedder.input_key] = xc
        return log
</code></pre>
<p>核心就是判断x到底是什么类型的数据后，来调用log_txt_as_img</p>
<pre><code class="language-python3">def log_txt_as_img(wh, xc, size=10):
    # wh a tuple of (width, height)
    # xc a list of captions to plot
    b = len(xc)
    txts = list()
    for bi in range(b):
        txt = Image.new(&quot;RGB&quot;, wh, color=&quot;white&quot;) # 白色背景新图像
        draw = ImageDraw.Draw(txt) # 创建为一个绘图对象
        font = ImageFont.truetype(&quot;data/DejaVuSans.ttf&quot;, size=size) # 指定字体
        nc = int(40 * (wh[0] / 256))
        if isinstance(xc[bi], list):
            text_seq = xc[bi][0]
        else:
            text_seq = xc[bi]
        lines = &quot;\n&quot;.join(
            text_seq[start : start + nc] for start in range(0, len(text_seq), nc)
        )
        
        # 在图上把字体画出来
        try:
            draw.text((0, 0), lines, fill=&quot;black&quot;, font=font)
        except UnicodeEncodeError:
            print(&quot;Cant encode string for logging. Skipping.&quot;)

        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0
        txts.append(txt)
    # 返回一个tensor格式的张量
    txts = np.stack(txts)
    txts = torch.tensor(txts)
    return txts
</code></pre>
<h3 id="log_images">log_images</h3>
<p>这个函数也是记录输入图像到log里面，比如记录了encode前是什么图像信息、直接decode又是什么图像信息，如果sample就再记录sample+decode后的信息。</p>
<pre><code class="language-python3">    @torch.no_grad()
    def log_images(
        self,
        batch: Dict,
        N: int = 8,
        sample: bool = True,
        ucg_keys: List[str] = None,
        **kwargs,
    ) -&gt; Dict:
        # 从embedder层获取key
        conditioner_input_keys = [e.input_key for e in self.conditioner.embedders]
        # 确定无条件生成的key(如果有)在前面获取的key里面
        if ucg_keys:
            assert all(map(lambda x: x in conditioner_input_keys, ucg_keys)), (
                &quot;Each defined ucg key for sampling must be in the provided conditioner input keys,&quot;
                f&quot;but we have {ucg_keys} vs. {conditioner_input_keys}&quot;
            )
        else:
            ucg_keys = conditioner_input_keys
        log = dict()

        x = self.get_input(batch)

        # 获取条件和无条件向量嵌入
        c, uc = self.conditioner.get_unconditional_conditioning(
            batch,
            force_uc_zero_embeddings=ucg_keys
            if len(self.conditioner.embedders) &gt; 0
            else [],
        )

        sampling_kwargs = {}

        N = min(x.shape[0], N)
        x = x.to(self.device)[:N]
        log[&quot;inputs&quot;] = x
        z = self.encode_first_stage(x) # 对输入图像进行编码
        log[&quot;reconstructions&quot;] = self.decode_first_stage(z) # 记录解码图像
        log.update(self.log_conditionings(batch, N)) # 更新日志(条件信息)
        
        # 将条件向量转换到合适的device
        for k in c:
            if isinstance(c[k], torch.Tensor):
                c[k], uc[k] = map(lambda y: y[k][:N].to(self.device), (c, uc))
        
        # 如果sample就采样生成结果并记录到log里面
        if sample:
            with self.ema_scope(&quot;Plotting&quot;):
                samples = self.sample(
                    c, shape=z.shape[1:], uc=uc, batch_size=N, **sampling_kwargs
                )
            samples = self.decode_first_stage(samples)
            log[&quot;samples&quot;] = samples
        return log
</code></pre>
<h3 id="model-ema">model EMA</h3>
<p>在DiffusionEngine初始化的时候还进行了ema初始化操作：</p>
<pre><code class="language-python3">if self.use_ema:
            self.model_ema = LitEma(self.model, decay=ema_decay_rate)
            print(f&quot;Keeping EMAs of {len(list(self.model_ema.buffers()))}.&quot;)
</code></pre>
<p>且还有一个上下文方法，ema_scope用于加载模型的ema参数</p>
<pre><code class="language-python3">    @contextmanager
    def ema_scope(self, context=None):
        if self.use_ema:
            self.model_ema.store(self.model.parameters())
            self.model_ema.copy_to(self.model)
            if context is not None:
                print(f&quot;{context}: Switched to EMA weights&quot;)
        try:
            yield None
        finally:
            if self.use_ema:
                self.model_ema.restore(self.model.parameters())
                if context is not None:
                    print(f&quot;{context}: Restored training weights&quot;)
</code></pre>
<p>所以我们需要了解一下EMA是什么，有什么作用。</p>
<h4 id="ema-移动指数平均">EMA 移动指数平均</h4>
<p>指数移动平均（Exponential Moving Average, EMA）是一种常用的技巧，可以帮助模型在训练过程中更稳定，通常用于提高模型的泛化能力和性能。<br>
这种技术在深度学习中被广泛应用，主要有以下几个原因:</p>
<p>1.降低噪声和波动<br>
在模型训练过程中，尤其是在使用随机梯度下降（SGD）或其变种时，参数更新可能会受到噪声和波动的影响。这些噪声和波动可能来自于小批量数据的不稳定性或学习率的变化。EMA 通过对参数进行加权平均，可以平滑掉这些噪声和波动，使得模型参数更加稳定。</p>
<p>2.缓解过拟合<br>
EMA 可以看作是一种正则化技术。通过对参数进行加权平均，EMA 可以抑制参数的过度波动，从而减少过拟合的风险。特别是在训练后期，EMA 可以帮助模型更好地泛化到未见过的数据。</p>
<p>3.提高模型的泛化能力<br>
由于 EMA 在一定程度上平滑了参数更新，它可以帮助模型更好地捕捉数据的总体趋势，而不是过度拟合到训练数据中的细节。这种平滑效应可以提高模型在测试集上的性能，从而提高模型的泛化能力。</p>
<p>4.减少参数的极端值<br>
EMA 可以防止参数出现极端值。极端值可能会导致模型的不稳定性和性能下降。通过对参数进行加权平均，EMA 可以减缓参数的剧烈变化，使得参数更加平滑和稳定。<br>
（来自于chatgpt）</p>
<p>我的理解就是有点类似于集成模型的原理，集成了训练以来所有轮参数模型的一个结果，移动平均就是对所有模型输出结果的一个处理。<br>
公式就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>e</mi><mi>m</mi><msub><mi>a</mi><mi>w</mi></msub><mi mathvariant="normal">​</mi><mo>=</mo><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mo>⋅</mo><mi>e</mi><mi>m</mi><msub><mi>a</mi><mi>w</mi></msub><mi mathvariant="normal">​</mi><mo>+</mo><mo>(</mo><mn>1</mn><mi mathvariant="normal">−</mi><mi>d</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>y</mi><mo>)</mo><mo>⋅</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">ema_w​=decay⋅ema_w​+(1−decay)⋅w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">m</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">​</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">m</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">​</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mord">−</span><span class="mord mathdefault">d</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span><br>
对应到源文件generative-models/sgm/modules/ema.py</p>
<pre><code class="language-python3">import torch
from torch import nn


class LitEma(nn.Module):
    def __init__(self, model, decay=0.9999, use_num_upates=True):
        super().__init__()
        if decay &lt; 0.0 or decay &gt; 1.0:
            raise ValueError(&quot;Decay must be between 0 and 1&quot;)

        self.m_name2s_name = {}
        
        # 为decay和num_updates注册buffer
        self.register_buffer(&quot;decay&quot;, torch.tensor(decay, dtype=torch.float32))
        self.register_buffer(
            &quot;num_updates&quot;,
            torch.tensor(0, dtype=torch.int)
            if use_num_upates
            else torch.tensor(-1, dtype=torch.int),
        )

        # 遍历原模型的可训练参数，为每个参数对应创建一个ema参数并注册buffer
        for name, p in model.named_parameters():
            if p.requires_grad:
                # remove as '.'-character is not allowed in buffers
                s_name = name.replace(&quot;.&quot;, &quot;&quot;)
                self.m_name2s_name.update({name: s_name})
                self.register_buffer(s_name, p.clone().detach().data)

        self.collected_params = []

    def reset_num_updates(self):
        # 重置num_updates
        del self.num_updates
        self.register_buffer(&quot;num_updates&quot;, torch.tensor(0, dtype=torch.int))

    def forward(self, model):
        decay = self.decay

        if self.num_updates &gt;= 0:
            self.num_updates += 1
            decay = min(self.decay, (1 + self.num_updates) / (10 + self.num_updates))

        one_minus_decay = 1.0 - decay

        with torch.no_grad():
            # m_param是原模型的参数
            # shadow_params是ema模型的参数
            m_param = dict(model.named_parameters())
            shadow_params = dict(self.named_buffers())

            for key in m_param:
                if m_param[key].requires_grad:
                    sname = self.m_name2s_name[key]
                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])
                    # ema_w = ema_w - (1 - decay)(ema_w - w)
                    #       = decay*ema_w + (1 - decay)*w
                    shadow_params[sname].sub_(
                        one_minus_decay * (shadow_params[sname] - m_param[key])
                    )
                else:
                    assert not key in self.m_name2s_name

    def copy_to(self, model):
        # 把ema参数复制到模型参数
        m_param = dict(model.named_parameters())
        shadow_params = dict(self.named_buffers())
        for key in m_param:
            if m_param[key].requires_grad:
                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)
            else:
                assert not key in self.m_name2s_name

    # 存储和重新载入模型参数
    def store(self, parameters):
        &quot;&quot;&quot;
        Save the current parameters for restoring later.
        Args:
          parameters: Iterable of `torch.nn.Parameter`; the parameters to be
            temporarily stored.
        &quot;&quot;&quot;
        self.collected_params = [param.clone() for param in parameters]

    def restore(self, parameters):
        &quot;&quot;&quot;
        Restore the parameters stored with the `store` method.
        Useful to validate the model with EMA parameters without affecting the
        original optimization process. Store the parameters before the
        `copy_to` method. After validation (or model saving), use this to
        restore the former parameters.
        Args:
          parameters: Iterable of `torch.nn.Parameter`; the parameters to be
            updated with the stored parameters.
        &quot;&quot;&quot;
        for c_param, param in zip(self.collected_params, parameters):
            param.data.copy_(c_param.data)
</code></pre>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#model">Model</a>
<ul>
<li><a href="#diffusionengine">DiffusionEngine</a>
<ul>
<li><a href="#_init_first_stage">_init_first_stage</a></li>
<li><a href="#forward">forward</a></li>
<li><a href="#train">train</a></li>
<li><a href="#configure_optimizers">configure_optimizers</a></li>
<li><a href="#sample">Sample</a></li>
<li><a href="#log_conditionings">log_conditionings</a></li>
<li><a href="#log_images">log_images</a></li>
<li><a href="#model-ema">model EMA</a>
<ul>
<li><a href="#ema-%E7%A7%BB%E5%8A%A8%E6%8C%87%E6%95%B0%E5%B9%B3%E5%9D%87">EMA 移动指数平均</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://ShiMinghao0208.github.io/post/sdxl-dai-ma-yue-du-xun-lian-pytorch-lightning-21/">
              <h3 class="post-title">
                SDXL代码阅读-训练(pytorch lightning)-2.1
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://ShiMinghao0208.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
